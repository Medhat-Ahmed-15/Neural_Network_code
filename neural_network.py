# -*- coding: utf-8 -*-
"""Neural_Network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/115VWY0RRFxX4QYjZl-dS6-vDQvTRmNhu
"""

import pandas as pd
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, cohen_kappa_score
from keras.models import Sequential
from keras.layers import Dense
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

"""# ** MultiClass Dataset**"""

dataFrame=pd.read_csv('Iris_data_set.csv')

#since 2n el data lama 2a3mlha print haytla3le tabel 2wl row 3obara 3an el data msh el labels w dah msh mantke ana 3ayz table feeh row lal titles w ba3d ked el data tahteeh 3ashan keda ana 3aml add la 4 columns bahot feehom titles
dataFrame.columns=['sepal length', 'sepal width', 'petal length', 'petal width', 'class']
dataFrame.head()

import numpy as np

#Now putting the features in variable 'X' and the classes in variable 'Y'
features = dataFrame.iloc[:,0:4].values# this means that it will take the columns from column 0 tp column 3  for the features excluding the column that contain 0,1,2,3
classes = dataFrame.iloc[:,-1].values# this means that it will take the last column for the class 

#this for encoding the out which is the classes which are just strings to numbers so we can use them for calculations
encoder = LabelEncoder()
encoder.fit(classes)
classesInNumbers = encoder.transform(classes)


#Now Splitting the dataset into the Training set and Test set
features_train, features_test, classes_train, classes_test = train_test_split(features, classesInNumbers, test_size=0.3, random_state=0)

#?????????????????????????????????????????????????????????????????

# Feature Scaling
sc_X = StandardScaler()
features_train = sc_X.fit_transform(features_train)
features_test = sc_X.transform(features_test)

features = features_train.astype('float')
classes = classes_train.astype('float')
classes = to_categorical(classes)

Xtest = features_test.astype('float')
Ytest = classes_test.astype('float')
Ytest = to_categorical(Ytest)

#?????????????????????????????????????????????????????????????????

# Going to use sequential model
classifier = Sequential()

# Adding the input layer and the first hidden layer
layer_info = Dense(activation='relu', input_dim=4, units=60)# units are the number of neurons in single layer
classifier.add(layer_info)


# Adding second hidden layer
layer_info = Dense(activation='relu', units=20)
classifier.add(layer_info)


# Adding output layer
layer_info = Dense(activation='sigmoid',units=3)# here in the output I just needed 3 neurons because I only have 3 classes
classifier.add(layer_info)


# Compiling the ANN
classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])# here in the compile we initialize the weights, specify any functions we're gonna use


# Training the model
classifier.fit(features, classes, batch_size=32, epochs=50)


# Predicting the Test set results
y_pred = classifier.predict(Xtest)#???????????????==> howa hna leih hat fal predict el Xtest

print(confusion_matrix(Ytest.argmax(axis=1), y_pred.argmax(axis=1).round()))
print(accuracy_score(Ytest.argmax(axis=1), y_pred.argmax(axis=1).round())*100)

"""# **Binary Dataset**"""

from sklearn import datasets
import numpy as np

cancer = datasets.load_breast_cancer()
features = cancer.data       #features
classes = cancer.target     #class

features.shape #mansash 2ashoof el dataset dee feeha kam feature 3ashan 3ala 2asaso hahot fal input_shape

from tensorflow.keras import optimizers

# Set up the network
#
network = Sequential()
network.add(Dense(32, activation='relu', input_shape=(30,)))
network.add(Dense(32, activation='relu'))
network.add(Dense(16, activation='relu'))
network.add(Dense(1, activation='sigmoid'))

network.compile(optimizer=optimizers.RMSprop(learning_rate=0.01),
                loss='binary_crossentropy',
                metrics=['accuracy'])

feature_train, feature_test, class_train, class_test = train_test_split(features, classes, test_size=0.3, stratify=classes, random_state=42)
feature_train, feature_val, class_train, class_val = train_test_split(feature_train, class_train, test_size=0.1, stratify=class_train, random_state=42) #?????????????==> howa msh el mafrood yab2 ma3aya hna bs el feature_val w el class_val


history = network.fit(feature_train, class_train,
                    validation_data=(feature_val, class_val),
                    epochs=90,
                    batch_size=20)

import matplotlib.pyplot as plt
 
history_dict = history.history
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']
accuracy = history_dict['accuracy']
val_accuracy = history_dict['val_accuracy']
 
epochs = range(1, len(loss_values) + 1)
fig, ax = plt.subplots(1, 2, figsize=(14, 6))
#
# Plot the model accuracy vs Epochs
#
ax[0].plot(epochs, accuracy, 'r', label='Training accuracy')
ax[0].plot(epochs, val_accuracy, 'b', label='Validation accuracy')
ax[0].set_title('Training & Validation Accuracy', fontsize=16)
ax[0].set_xlabel('Epochs', fontsize=16)
ax[0].set_ylabel('Accuracy', fontsize=16)
ax[0].legend()
#
# Plot the loss vs Epochs
#
ax[1].plot(epochs, loss_values, 'r', label='Training loss')
ax[1].plot(epochs, val_loss_values, 'b', label='Validation loss')
ax[1].set_title('Training & Validation Loss', fontsize=16)
ax[1].set_xlabel('Epochs', fontsize=16)
ax[1].set_ylabel('Loss', fontsize=16)
ax[1].legend()

# Predicting the Test set results
y_pred = network.predict(feature_test).round()
print(confusion_matrix(class_test, y_pred))
print(accuracy_score(class_test, y_pred)*100)